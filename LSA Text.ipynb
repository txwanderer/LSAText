{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis - Week 4 - Data Science - Jared Knowles\n",
    "\n",
    "## NOTE:  This version of Latent Semantic Analysis is a modified copy of the original provided by Michael Bernico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added import for fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/keri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this only once\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following cell modified to import data from \"alt.atheism\" newgroup instead of 'raw_forum_posts.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title'\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class', 'nntp', \n",
    "                'posting', 're', 'host', 'jon', 'keith', 'livesey', 'organization', 'kent', 'sandvik', 'cobb', 'cs', 'solntze', \n",
    "                'schneider', 'sgi', 'ico', 'com', 'tek', 'edu', 'wpd', 'de', 'uk', 'caltech', 'cc', 'kmr4', 'uiuc', \n",
    "                'isn', 'robert', 'beauchaine', 'osrhe', 'okcforum', 'frank', 'dwyer', 'bobbe', 'subject', 've', 'conner', 'benedikt',\n",
    "                'cwru', 'jaeger', 'cco', 'dbstu1', 'rz', 'tu', 'alexia', 'lis', 'alt', 'us', 'bu', 'university', 'monash', 'faq',\n",
    "                'allan', 'co', 'bill', 'd012s658', 'swinburne', 'would', 'however', 'au', 'jim', 'could', 'gregg', 'po', 'rh',\n",
    "                'fi', 'oulu', 'distribution', 'statement', 'etc', 'perry', 'mantis', 'well', 'll', 'whether', 'either', 'like',\n",
    "                'yet', 'also', 'really', 'actually', 'go', 'get', 'much', 'rather', 'perhaps', 'reply', 'yes', 'article', 'dan', ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "\n",
    "I'm going to use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset,\n",
    "                                 use_idf=True, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x197801 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9053)\t0.0343411268565\n",
      "  (0, 161372)\t0.0343411268565\n",
      "  (0, 76994)\t0.0343411268565\n",
      "  (0, 177782)\t0.0343411268565\n",
      "  (0, 6168)\t0.0364813736275\n",
      "  (0, 5844)\t0.0364813736275\n",
      "  (0, 5692)\t0.0364813736275\n",
      "  (0, 190905)\t0.0364813736275\n",
      "  (0, 98047)\t0.0364813736275\n",
      "  (0, 122998)\t0.0343411268565\n",
      "  (0, 187555)\t0.0343411268565\n",
      "  (0, 43669)\t0.0343411268565\n",
      "  (0, 93486)\t0.0343411268565\n",
      "  (0, 9636)\t0.0343411268565\n",
      "  (0, 9620)\t0.0386788868944\n",
      "  (0, 146342)\t0.0419221813194\n",
      "  (0, 26100)\t0.0386788868944\n",
      "  (0, 34945)\t0.0434619997185\n",
      "  (0, 14562)\t0.0434619997185\n",
      "  (0, 61319)\t0.0386788868944\n",
      "  (0, 64428)\t0.0386788868944\n",
      "  (0, 119971)\t0.0386788868944\n",
      "  (0, 48162)\t0.0386788868944\n",
      "  (0, 4920)\t0.0386788868944\n",
      "  (0, 37984)\t0.0343411268565\n",
      "  :\t:\n",
      "  (0, 97520)\t0.0291126612461\n",
      "  (0, 147735)\t0.0204065399088\n",
      "  (0, 49741)\t0.0406640580797\n",
      "  (0, 78480)\t0.0241172063404\n",
      "  (0, 194937)\t0.00868177165955\n",
      "  (0, 112078)\t0.0272361627501\n",
      "  (0, 46326)\t0.031391518164\n",
      "  (0, 119190)\t0.0229644374089\n",
      "  (0, 1303)\t0.0482451125427\n",
      "  (0, 31082)\t0.0482451125427\n",
      "  (0, 5491)\t0.0378661164409\n",
      "  (0, 100946)\t0.00690057315139\n",
      "  (0, 38689)\t0.0266084053399\n",
      "  (0, 104719)\t0.0338957740703\n",
      "  (0, 24013)\t0.0257623790478\n",
      "  (0, 8358)\t0.0308161372721\n",
      "  (0, 163082)\t0.0686822537129\n",
      "  (0, 112876)\t0.0343411268565\n",
      "  (0, 186543)\t0.0334774294326\n",
      "  (0, 50074)\t0.0244382211113\n",
      "  (0, 182353)\t0.0348172172131\n",
      "  (0, 182362)\t0.0348172172131\n",
      "  (0, 104722)\t0.062783036328\n",
      "  (0, 104695)\t0.0565997815852\n",
      "  (0, 9049)\t0.062783036328\n"
     ]
    }
   ],
   "source": [
    "#After\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###LSA\n",
    "\n",
    "Input:  X, a matrix where m is the number of documents I have, and n is the number of terms.\n",
    "\n",
    "Process:   I'm going to decompose X into three matricies called U, S, and T.  When we do the decomposition, we have to pick a value k, that's how many concepts we are going to keep.  \n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "U will be a m x k matrix.  The rows will be documents and the columns will be 'concepts'\n",
    "\n",
    "S will be a k x k diagnal matrix.   The elements will be the amount of variation captured from each concept.\n",
    "\n",
    "V will be a m x k (mind the transpose) matrix.   The rows will be terms and the columns will be conepts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799, 197801)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00175525,  0.0004119 ,  0.0004119 , ...,  0.00045795,\n",
       "        0.00045795,  0.00045795])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "god\n",
      "one\n",
      "people\n",
      "think\n",
      "writes\n",
      "say\n",
      "morality\n",
      "believe\n",
      "atheism\n",
      "moral\n",
      " \n",
      "Concept 1:\n",
      "theism\n",
      "fanatism\n",
      "belief\n",
      "irrational\n",
      "rational\n",
      "evidence\n",
      "many\n",
      "fanaticism\n",
      "people\n",
      "say\n",
      " \n",
      "Concept 2:\n",
      "god\n",
      "islam\n",
      "religion\n",
      "liar\n",
      "atheists\n",
      "atheism\n",
      "said\n",
      "lunatic\n",
      "bible\n",
      "jesus\n",
      " \n",
      "Concept 3:\n",
      "god\n",
      "books\n",
      "know\n",
      "press\n",
      "made\n",
      "prometheus\n",
      "prometheus books\n",
      "often\n",
      "history\n",
      "existence god\n",
      " \n",
      "Concept 4:\n",
      "god\n",
      "know\n",
      "see\n",
      "reason\n",
      "say\n",
      "morality\n",
      "vice\n",
      "never\n",
      "moral\n",
      "take\n",
      " \n",
      "Concept 5:\n",
      "time\n",
      "morality\n",
      "moral\n",
      "society\n",
      "people\n",
      "case\n",
      "atheists\n",
      "saying\n",
      "right\n",
      "need\n",
      " \n",
      "Concept 6:\n",
      "god\n",
      "atheism\n",
      "way\n",
      "religious\n",
      "atheists\n",
      "thing\n",
      "cannot\n",
      "faith\n",
      "atheist\n",
      "things\n",
      " \n",
      "Concept 7:\n",
      "god\n",
      "atheists\n",
      "others\n",
      "faith\n",
      "real\n",
      "far\n",
      "good\n",
      "different\n",
      "back\n",
      "agree\n",
      " \n",
      "Concept 8:\n",
      "atheists\n",
      "many\n",
      "people\n",
      "may\n",
      "religion\n",
      "jesus\n",
      "god\n",
      "must\n",
      "take\n",
      "matthew\n",
      " \n",
      "Concept 9:\n",
      "jesus\n",
      "islamic\n",
      "moral\n",
      "bible\n",
      "matthew\n",
      "thought\n",
      "something\n",
      "make\n",
      "religious\n",
      "said\n",
      " \n",
      "Concept 10:\n",
      "must\n",
      "time\n",
      "jesus\n",
      "things\n",
      "religion\n",
      "mean\n",
      "one\n",
      "matthew\n",
      "peace\n",
      "person\n",
      " \n",
      "Concept 11:\n",
      "believe\n",
      "god\n",
      "islam\n",
      "many\n",
      "moral\n",
      "know\n",
      "say\n",
      "atheists\n",
      "real\n",
      "rushdie\n",
      " \n",
      "Concept 12:\n",
      "atheists\n",
      "writes\n",
      "atheism\n",
      "people\n",
      "person\n",
      "bible\n",
      "even\n",
      "post\n",
      "objective\n",
      "death\n",
      " \n",
      "Concept 13:\n",
      "moral\n",
      "know\n",
      "good\n",
      "exist\n",
      "people\n",
      "read\n",
      "believe\n",
      "objective\n",
      "reason\n",
      "atheist\n",
      " \n",
      "Concept 14:\n",
      "even\n",
      "moral\n",
      "system\n",
      "take\n",
      "evidence\n",
      "made\n",
      "question\n",
      "book\n",
      "seem\n",
      "everything\n",
      " \n",
      "Concept 15:\n",
      "morality\n",
      "atheism\n",
      "believe\n",
      "must\n",
      "think\n",
      "writes\n",
      "see\n",
      "jesus\n",
      "true\n",
      "seems\n",
      " \n",
      "Concept 16:\n",
      "atheism\n",
      "true\n",
      "one\n",
      "argument\n",
      "anyone\n",
      "different\n",
      "typical\n",
      "seem\n",
      "good\n",
      "information\n",
      " \n",
      "Concept 17:\n",
      "way\n",
      "seem\n",
      "atheism\n",
      "know\n",
      "something\n",
      "religious\n",
      "mean\n",
      "one\n",
      "say\n",
      "lot\n",
      " \n",
      "Concept 18:\n",
      "atheists\n",
      "one\n",
      "belief\n",
      "religious\n",
      "may\n",
      "read\n",
      "religion\n",
      "different\n",
      "question\n",
      "atheist\n",
      " \n",
      "Concept 19:\n",
      "say\n",
      "someone\n",
      "must\n",
      "different\n",
      "case\n",
      "reason\n",
      "god\n",
      "moral\n",
      "muslim\n",
      "belief\n",
      " \n",
      "Concept 20:\n",
      "religion\n",
      "vice\n",
      "right\n",
      "science\n",
      "must\n",
      "good\n",
      "didn\n",
      "things\n",
      "rushdie\n",
      "islam\n",
      " \n",
      "Concept 21:\n",
      "christian\n",
      "religion\n",
      "people\n",
      "good\n",
      "time\n",
      "another\n",
      "way\n",
      "islam\n",
      "group\n",
      "argument\n",
      " \n",
      "Concept 22:\n",
      "believe\n",
      "say\n",
      "see\n",
      "think\n",
      "murder\n",
      "look\n",
      "agree\n",
      "many\n",
      "people\n",
      "human\n",
      " \n",
      "Concept 23:\n",
      "moral\n",
      "atheists\n",
      "since\n",
      "must\n",
      "faith\n",
      "first\n",
      "evidence\n",
      "someone\n",
      "time\n",
      "years\n",
      " \n",
      "Concept 24:\n",
      "religion\n",
      "good\n",
      "wrong\n",
      "many\n",
      "bible\n",
      "want\n",
      "see\n",
      "find\n",
      "without\n",
      "part\n",
      " \n",
      "Concept 25:\n",
      "moral\n",
      "say\n",
      "make\n",
      "religion\n",
      "argument\n",
      "jesus\n",
      "life\n",
      "right\n",
      "book\n",
      "little\n",
      " \n",
      "Concept 26:\n",
      "god\n",
      "read\n",
      "think\n",
      "morality\n",
      "religion\n",
      "time\n",
      "little\n",
      "life\n",
      "used\n",
      "right\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print \"Concept %d:\" % i\n",
    "    for term in sortedTerms:\n",
    "        print term[0]\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### At this point, it would appear at a basic level, if your \"computer\" or researcher at the moment didn't exactly know what atheism is all about, the observer would at least know by now that it has something to do with religion, god, morality, rational vs. irrational, etc.  Maybe by further fine tuning the stop words, one could get to a much closer view of what atheism is, but at least its getting into its realm. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
